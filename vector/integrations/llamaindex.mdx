---
title: "LlamaIndex with Upstash Vector"
sidebarTitle: "LlamaIndex"
---

You can use LlamaIndex with Upstash Vector to perform Retrieval-Augmented Generation (RAG). LlamaIndex is a powerful tool that integrates seamlessly with vector databases like Upstash Vector, enabling advanced query and response capabilities.

## Install

```bash
pip install llama-index upstash-vector llama-index-vector-stores-upstash python-dotenv
```

## Setup

First, create a Vector Index in the [Upstash Console](https://console.upstash.com). Configure the index with:
- **Dimensions**: 1536
- **Distance Metric**: Cosine

Once the index is created, copy the `UPSTASH_VECTOR_REST_URL` and `UPSTASH_VECTOR_REST_TOKEN` and add them to your `.env` file along with your OpenAI API key:

```
UPSTASH_VECTOR_REST_URL=your_upstash_url
UPSTASH_VECTOR_REST_TOKEN=your_upstash_token
OPENAI_API_KEY=your_openai_api_key
```

## Usage

Hereâ€™s how you can integrate LlamaIndex with Upstash Vector:

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.vector_stores.upstash import UpstashVectorStore
from llama_index.core import StorageContext
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set OpenAI API key
openai.api_key = os.environ["OPENAI_API_KEY"]

# Initialize Upstash Vector store
upstash_vector_store = UpstashVectorStore(
    url=os.environ["UPSTASH_VECTOR_REST_URL"],
    token=os.environ["UPSTASH_VECTOR_REST_TOKEN"],
)

# Load documents using SimpleDirectoryReader
documents = SimpleDirectoryReader("./documents/").load_data()

# Create a storage context and initialize the index
storage_context = StorageContext.from_defaults(vector_store=upstash_vector_store)
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)
```

## Querying

Once the index is created, you can query it to retrieve and generate responses based on document content.

```python
# Initialize the query engine
query_engine = index.as_query_engine()

# Perform queries
response_1 = query_engine.query("What is global warming?")
print(response_1)

response_2 = query_engine.query("How can we reduce our carbon footprint?")
print(response_2)
```

## Notes

- You can specify a namespace when creating the `UpstashVectorStore` instance:
  ```python
  vector_store = UpstashVectorStore(
      url="your_upstash_url",
      token="your_upstash_token",
      namespace="your_namespace"
  )
  ```

- Visit the [LlamaIndex documentation](https://docs.llamaindex.ai/en/latest) for more details.